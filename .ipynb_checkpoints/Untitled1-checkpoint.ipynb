{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c423299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79a2c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d74d59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "preprocessing_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82a6c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(preprocessing_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "087a9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoder = hub.KerasLayer(encoder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac31218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b02d42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() ##Only minor data is missing nut still\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6d62e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000 #no of samples for training\n",
    "df = df.sample(n,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fdbde",
   "metadata": {},
   "source": [
    "## Balancing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fb6cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.62\n",
       "1    0.38\n",
       "Name: is_duplicate, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_duplicate.value_counts()/df.is_duplicate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cee585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_du = df[df.is_duplicate==1]\n",
    "df_ndu = df[df.is_duplicate==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e73ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndu = df_ndu.sample(df_du.shape[0])\n",
    "df = pd.concat([df_du,df_ndu],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be832fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5\n",
       "0    0.5\n",
       "Name: is_duplicate, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_duplicate.value_counts()/df.is_duplicate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bf08db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cba053af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(q):\n",
    "    q = str(q).lower().strip()\n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    # Decontracting words\n",
    "    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "    # https://stackoverflow.com/a/19794953\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "    return q\n",
    "\n",
    "    \n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e4f7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "705746d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.concat([df['question1'],df['question2'],df['is_duplicate']],axis=1)\n",
    "# df_.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc62f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "##applying preprocessing\n",
    "df_['question1']=df_['question1'].apply(preprocessing)\n",
    "\n",
    "df_['question2']=df_['question2'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f86e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    from nltk.corpus import stopwords \n",
    "    sw = stopwords.words(\"english\")\n",
    "    features = [0.0]*10\n",
    "    q1_tokens = word_tokenize(q1)#tokenization\n",
    "    q2_tokens = word_tokenize(q2)\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return features\n",
    "    common_token_count = len(list([tk for tk in q1_tokens if tk in q2_tokens]))\n",
    "    \n",
    "    q1_words = list([word for word in q1_tokens if word not in sw])##words\n",
    "    q2_words = list([word for word in q2_tokens if word not in sw])\n",
    "    \n",
    "    q1_stopword = list([word for word in q1_tokens if word in sw])##stopwords\n",
    "    q2_stopword = list([word for word in q2_tokens if word in sw])\n",
    "    \n",
    "\n",
    "    common_word_count = len(list([word for word in q1_words if word in q2_words]))\n",
    "    common_stopword_count = len(list([stopword for stopword in q1_stopword if stopword in q2_stopword]))\n",
    "    common_token_count = len(list([token for token in q1_tokens if token in q2_tokens]))\n",
    "    \n",
    "    features[0] = abs(len(q1_tokens)-len(q2_tokens))#lendiff\n",
    "    features[1] = (len(q2_tokens)+len(q1_tokens))/2 #meanlen\n",
    "    \n",
    "    features[2] = common_word_count / (min(len(q1_words), len(q2_words)) + 0.0001) #mincommonwords\n",
    "    features[3] = common_word_count / (max(len(q1_words), len(q2_words)) + 0.0001) #maxcommonwords\n",
    "    features[4] = common_stopword_count / (min(len(q1_stopword), len(q2_stopword)) + 0.0001)#mincommonstopword\n",
    "    features[5] = common_stopword_count / (max(len(q1_stopword), len(q2_stopword)) + 0.0001)#maxcommonsw\n",
    "    features[6] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + 0.0001)#mintk\n",
    "    features[7] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + 0.0001)#maxtk\n",
    "    \n",
    "    features[8] = int(q1_tokens[-1] == q2_tokens[-1])#lastwordequal\n",
    "    features[9] = int(q1_tokens[0] == q2_tokens[0]) #firstwordequal\n",
    "    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49ace93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_.apply(process_features, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e876467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[\"len1\"]  = list(map(lambda x: x[0], features))\n",
    "df_[\"len2\"]  = list(map(lambda x: x[1], features))\n",
    "df_[\"cwc_min\"]  = list(map(lambda x: x[2], features))\n",
    "df_[\"cwc_max\"]  = list(map(lambda x: x[3], features))\n",
    "df_[\"csc_min\"]  = list(map(lambda x: x[4], features))\n",
    "df_[\"csc_max\"]  = list(map(lambda x: x[5], features))\n",
    "df_[\"ctc_min\"]  = list(map(lambda x: x[6], features))\n",
    "df_[\"ctc_max\"]  = list(map(lambda x: x[7], features))\n",
    "df_[\"last_word_eq\"]  = list(map(lambda x: x[8], features))\n",
    "df_[\"first_word_eq\"]  = list(map(lambda x: x[9], features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7600f37",
   "metadata": {},
   "source": [
    "## Fuzzy Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a460c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fuzzy_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    fuzzy_features = [0.0]*7\n",
    "    \n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "    \n",
    "    #W_ratio\n",
    "    fuzzy_features[1] = fuzz.WRatio(q1,q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[2] = fuzz.partial_ratio(q1, q2)\n",
    "    \n",
    "    #partial_token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.partial_token_set_ratio(q1, q2)\n",
    "    \n",
    "    #partial_token_sort_ratio\n",
    "    fuzzy_features[4]=fuzz.partial_token_sort_ratio(q1,q2)\n",
    "    \n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[5] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[6] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ec6b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_features = df_.apply(process_fuzzy_features, axis=1)\n",
    "\n",
    "# Creating new feature columns for fuzzy features\n",
    "df_['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "df_['w_fuzz_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "df_['fuzz_partial_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "df_['partial_token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))\n",
    "df_['partial_token_sort_ratio'] = list(map(lambda x: x[4], fuzzy_features))\n",
    "\n",
    "\n",
    "df_['token_set_ratio'] = list(map(lambda x: x[5], fuzzy_features))\n",
    "df_['token_sort_ratio'] = list(map(lambda x: x[6], fuzzy_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569eb12d",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "949be134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question1', 'question2', 'is_duplicate', 'len1', 'len2', 'cwc_min',\n",
       "       'cwc_max', 'csc_min', 'csc_max', 'ctc_min', 'ctc_max', 'last_word_eq',\n",
       "       'first_word_eq', 'fuzz_ratio', 'w_fuzz_ratio', 'fuzz_partial_ratio',\n",
       "       'partial_token_set_ratio', 'partial_token_sort_ratio',\n",
       "       'token_set_ratio', 'token_sort_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3ed73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(df_.drop(['question1','question2','is_duplicate'],axis=1))\n",
    "\n",
    "df_scaled = pd.DataFrame(x_scaled,columns = ['len1', 'len2', 'cwc_min',\n",
    "       'cwc_max', 'csc_min', 'csc_max', 'ctc_min', 'ctc_max', 'last_word_eq',\n",
    "       'first_word_eq', 'fuzz_ratio', 'w_fuzz_ratio', 'fuzz_partial_ratio',\n",
    "       'partial_token_set_ratio', 'partial_token_sort_ratio',\n",
    "       'token_set_ratio', 'token_sort_ratio'],index=df_.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "392d4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad1c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e90faba",
   "metadata": {},
   "source": [
    "## Embedding using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a044ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = list(df_['question1']) + list(df_['question2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c04cb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(text_input):\n",
    "    preprocessed_text = bert_preprocess_model(text_input)\n",
    "    return bert_encoder(preprocessed_text)['pooled_output']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587c187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72ca9051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"keras_layer_3\" (type KerasLayer).\n\nGraph execution error:\n\nOOM when allocating tensor with shape[4560,12,128,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node transformer/layer_4/self_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_restored_function_body_106860]\n\nCall arguments received:\n  • inputs={'input_mask': 'tf.Tensor(shape=(4560, 128), dtype=int32)', 'input_type_ids': 'tf.Tensor(shape=(4560, 128), dtype=int32)', 'input_word_ids': 'tf.Tensor(shape=(4560, 128), dtype=int32)'}\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_792/2505706237.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mq1_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_sentence_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_792/170736919.py\u001b[0m in \u001b[0;36mget_sentence_embedding\u001b[1;34m(text_input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sentence_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpreprocessed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_preprocess_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbert_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pooled_output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;31m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m       result = smart_cond.smart_cond(training,\n\u001b[0m\u001b[0;32m    238\u001b[0m                                      \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                                      lambda: f(training=False))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    237\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[0;32m    238\u001b[0m                                      \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                                      lambda: f(training=False))\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"keras_layer_3\" (type KerasLayer).\n\nGraph execution error:\n\nOOM when allocating tensor with shape[4560,12,128,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node transformer/layer_4/self_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_restored_function_body_106860]\n\nCall arguments received:\n  • inputs={'input_mask': 'tf.Tensor(shape=(4560, 128), dtype=int32)', 'input_type_ids': 'tf.Tensor(shape=(4560, 128), dtype=int32)', 'input_word_ids': 'tf.Tensor(shape=(4560, 128), dtype=int32)'}\n  • training=False"
     ]
    }
   ],
   "source": [
    "q1_arr, q2_arr = np.vsplit(get_sentence_embedding(questions).toarray(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671992c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "df1 = pd.DataFrame(q1_arr, index= df_.index)\n",
    "df2 = pd.DataFrame(q2_arr, index= df_.index)\n",
    "df2.columns=range(3000,6000,1)\n",
    "df_vectors = pd.concat([df1,df2], axis=1,)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa22ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x_df = pd.concat([df_vectors,df_scaled],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a6ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f3a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d34b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
